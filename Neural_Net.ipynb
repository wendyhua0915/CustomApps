{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wendyhua0915/CustomApps/blob/master/Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "C1g2cj_5zzkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "73c7ef31-c6cc-4445-d060-4b2c0e4bf3bd"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tAgA69uo0l56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2518bf4d-36e8-418c-8992-15e1d3568d3c"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/My Drive/cs189"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/cs189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DfsTecVK1t6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb32bc0e-f19a-405b-9574-310f6bcd0ae9"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/cs189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n6iDnGc7z3Sq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "1c629743-dcb3-44a3-d588-b642fe122c5d"
      },
      "cell_type": "code",
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/43/c3c2e866a8803e196d6209595020a4a6db1a3c5d07c01455669497ae23d0/ipdb-0.12.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (40.9.0)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.16)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.7.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (1.11.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/59/24/91/695211bd228d40fb22dff0ce3f05ba41ab724ab771736233f3\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "34W7C5XFz8tD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d545b2cd-c4fe-4bd1-d118-3f04fb6a4f0b"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "from mds189 import Mds189\n",
        "import numpy as np\n",
        "from skimage import io, transform\n",
        "import ipdb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# Helper functions for loading images.\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def accimage_loader(path):\n",
        "    import accimage\n",
        "    try:\n",
        "        return accimage.Image(path)\n",
        "    except IOError:\n",
        "        # Potentially a decoding problem, fall back to PIL.Image\n",
        "        return pil_loader(path)\n",
        "\n",
        "def default_loader(path):\n",
        "    from torchvision import get_image_backend\n",
        "    if get_image_backend() == 'accimage':\n",
        "        return accimage_loader(path)\n",
        "    else:\n",
        "        return pil_loader(path)\n",
        "\n",
        "# flag for whether you're training or not\n",
        "is_train = True\n",
        "is_key_frame = True # TODO: set this to false to train on the video frames, instead of the key frames\n",
        "model_to_load = 'model.ckpt' # This is the model to load during testing, if you want to eval a previously-trained model.\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "#cudnn.benchmark = True\n",
        "\n",
        "# Parameters for data loader\n",
        "params = {'batch_size': 32,  # TODO: fill in the batch size. often, these are things like 32,64,128,or 256\n",
        "          'shuffle': True,\n",
        "          'num_workers': 2\n",
        "          }\n",
        "# TODO: Hyper-parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "# NOTE: depending on your optimizer, you may want to tune other hyperparameters as well\n",
        "\n",
        "# Datasets\n",
        "# TODO: put the path to your train, test, validation txt files\n",
        "if is_key_frame:\n",
        "    label_file_train =  './dataloader_files/keyframe_data_train.txt'\n",
        "    label_file_val  =  './dataloader_files/keyframe_data_val.txt'\n",
        "    # NOTE: the kaggle competition test data is only for the video frames, not the key frames\n",
        "    # this is why we don't have an equivalent label_file_test with keyframes\n",
        "else:\n",
        "    label_file_train = './dataloader_files/videoframe_data_train.txt'\n",
        "    label_file_val = './dataloader_files/videoframe_data_val.txt'\n",
        "    label_file_test = './dataloader_files/videoframe_data_test.txt'\n",
        "\n",
        "# TODO: you should normalize based on the average image in the training set. This shows\n",
        "# an example of doing normalization\n",
        "mean = [134.010302198/255, 118.599587912/255, 102.038804945/255]\n",
        "std = [23.5033438916/255, 23.8827343458/255, 24.5498666589/255]\n",
        "# TODO: if you want to pad or resize your images, you can put the parameters for that below.\n",
        "\n",
        "# Generators\n",
        "# NOTE: if you don't want to pad or resize your images, you should delete the Pad and Resize\n",
        "# transforms from all three _dataset definitions.\n",
        "train_dataset = Mds189(label_file_train,loader=default_loader,transform=transforms.Compose([\n",
        "                                          #     transforms.Pad(requires_parameters),    # TODO: if you want to pad your images\n",
        "                                          #     transforms.Resize(requires_parameters), # TODO: if you want to resize your images\n",
        "                                               transforms.ToTensor(),\n",
        "                                               transforms.Normalize(mean, std)\n",
        "                                           ]))\n",
        "train_loader = data.DataLoader(train_dataset, **params)\n",
        "\n",
        "val_dataset = Mds189(label_file_val,loader=default_loader,transform=transforms.Compose([\n",
        "                                            #   transforms.Pad(),\n",
        "                                            #   transforms.Resize(),\n",
        "                                               transforms.ToTensor(),\n",
        "                                               transforms.Normalize(mean, std)\n",
        "                                           ]))\n",
        "val_loader = data.DataLoader(val_dataset, **params)\n",
        "\n",
        "if not is_key_frame:\n",
        "    test_dataset = Mds189(label_file_test,loader=default_loader,transform=transforms.Compose([\n",
        "                                                   transforms.Pad(),\n",
        "                                                   transforms.Resize(),\n",
        "                                                   transforms.ToTensor(),\n",
        "                                                   transforms.Normalize(mean, std)\n",
        "                                               ]))\n",
        "    test_loader = data.DataLoader(test_dataset, **params)\n",
        "\n",
        "model = NeuralNet().to(device)\n",
        "\n",
        "# if we're only testing, we don't want to train for any epochs, and we want to load a model\n",
        "if not is_train:\n",
        "    num_epochs = 0\n",
        "    model.load_state_dict(torch.load('model.ckpt'))\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()#TODO: define your loss here. hint: should just require calling a built-in pytorch layer.\n",
        "# NOTE: you can use a different optimizer besides Adam, like RMSProp or SGD, if you'd like\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "# Loop over epochs\n",
        "print('Beginning training..')\n",
        "total_step = len(train_loader)\n",
        "#Own code below\n",
        "loss_list = []\n",
        "#Own code above\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    print('epoch {}'.format(epoch))\n",
        "    for i, (local_batch,local_labels) in enumerate(train_loader):\n",
        "        # Transfer to GPU\n",
        "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model.forward(local_ims)\n",
        "        loss = criterion(outputs, local_labels)\n",
        "        # TODO: maintain a list of your losses as a function of number of steps\n",
        "        #       because we ask you to plot this information\n",
        "        # NOTE: if you use Google Colab's tensorboard-like feature to visualize\n",
        "        #       the loss, you do not need to plot it here. just take a screenshot\n",
        "        #       of the loss curve and include it in your write-up.\n",
        "        loss_list.append(loss)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 4 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "end = time.time()\n",
        "print('Time: {}'.format(end - start))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "print('Beginning Testing..')\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predicted_list = []\n",
        "    groundtruth_list = []\n",
        "    for (local_batch,local_labels) in test_loader:\n",
        "        # Transfer to GPU\n",
        "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "\n",
        "        outputs = model.forward(local_ims)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += local_labels.size(0)\n",
        "        predicted_list.extend(predicted)\n",
        "        groundtruth_list.extend(local_labels)\n",
        "        correct += (predicted == local_labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
        "\n",
        "# Look at some things about the model results..\n",
        "# convert the predicted_list and groundtruth_list Tensors to lists\n",
        "pl = [p.cpu().numpy().tolist() for p in predicted_list]\n",
        "gt = [p.cpu().numpy().tolist() for p in groundtruth_list]\n",
        "\n",
        "# TODO: use pl and gt to produce your confusion matrices\n",
        "\n",
        "# view the per-movement accuracy\n",
        "label_map = ['reach','squat','inline','lunge','hamstrings','stretch','deadbug','pushup']\n",
        "for id in range(len(label_map)):\n",
        "    print('{}: {}'.format(label_map[id],sum([p and g for (p,g) in zip(np.array(pl)==np.array(gt),np.array(gt)==id)])/(sum(np.array(gt)==id)+0.)))\n",
        "\n",
        "# TODO: you'll need to run the forward pass on the kaggle competition images, and save those results to a csv file.\n",
        "if not is_key_frame:\n",
        "    # your code goes here!\n",
        "    pass\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning training..\n",
            "epoch 0\n",
            "Epoch [1/10], Step [4/91], Loss: 2.0834\n",
            "Epoch [1/10], Step [8/91], Loss: 2.3526\n",
            "Epoch [1/10], Step [12/91], Loss: 2.0804\n",
            "Epoch [1/10], Step [16/91], Loss: 2.0701\n",
            "Epoch [1/10], Step [20/91], Loss: 2.0711\n",
            "Epoch [1/10], Step [24/91], Loss: 2.0653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F3g97-Dw0ADa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO: one way of defining your model architecture is to fill in a class like NeuralNet()\n",
        "# NOTE: you should not overwrite the models you try whose performance you're keeping track of.\n",
        "#       one thing you could do is have many different model forward passes in class NeuralNet()\n",
        "#       and then depending on which model you want to train/evaluate, you call that model's\n",
        "#       forward pass. this strategy will save you a lot of time in the long run. the last thing\n",
        "#       you want to do is have to recode the layer structure for a model (whose performance\n",
        "#       you're reporting) because you forgot to e.g., compute the confusion matrix on its results\n",
        "#       or visualize the error modes of your (best) model\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # you can define some common layers, for example:\n",
        "        #self.conv1 = nn.Conv2d(3, 6, 5) # you should review the definition of nn.Conv2d online\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 4, 5)\n",
        "        self.conv3 = nn.Conv2d(4, 7, 5)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # note: input_dimensions and output_dimensions are not defined, they\n",
        "        # are placeholders to show you what arguments to pass to nn.Linear\n",
        "        self.fc1 = nn.Linear(7*52*24, 300)\n",
        "        self.fc2 = nn.Linear(300, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # now you can use the layers you defined, to write the forward pass, i.e.,\n",
        "        # network architecture for your model\n",
        "        x = self.pool(F.relu(self.conv1(x))) # x -> convolution -> ReLU -> max pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        # Tensors need to be reshaped before going into an fc layer\n",
        "        # the -1 will correspond to the batch size\n",
        "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3]) #what is num_elements_in_one_x_sample\n",
        "        x = F.relu(self.fc1(x)) # x -> fc (affine) layer -> relu\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b8Gz7rhW0EE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}